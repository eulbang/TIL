# ìì—°ì–´ì²˜ë¦¬ ë° í…ìŠ¤íŠ¸ íŒŒìš´ë°ì´ì…˜ ëª¨ë¸
## ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸

---

## ëª©ì°¨
1. í…ìŠ¤íŠ¸ íŒŒìš´ë°ì´ì…˜ ëª¨ë¸ ì‚´í´ë³´ê¸°
   - í…ìŠ¤íŠ¸ íŒŒìš´ë°ì´ì…˜ ëª¨ë¸ (ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸)ì´ë€?
   - ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸ ì˜ˆì‹œ
2. ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸ì˜ í•™ìŠµ
   - ì§€ì‹œ í•™ìŠµ
   - ì„ í˜¸ í•™ìŠµ
3. ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸ì˜ ì¶”ë¡ 
   - ë””ì½”ë”©
   - í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§
4. ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸ì˜ í‰ê°€ì™€ ì‘ìš©
   - ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸ì˜ í‰ê°€
   - ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸ì˜ ì‘ìš©/í•œê³„

---

## í•™ìŠµ ëª©í‘œ
- í…ìŠ¤íŠ¸ íŒŒìš´ë°ì´ì…˜ ëª¨ë¸(ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸)ì´ ë¬´ì—‡ì¸ì§€ ì´í•´í•©ë‹ˆë‹¤.
- ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸ì˜ í•™ìŠµ ë° ì¶”ë¡  ë°©ì‹ì„ ì´í•´í•©ë‹ˆë‹¤.
- ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸ì˜ ì‘ìš© ë° í•œê³„ë¥¼ íŒŒì•…í•˜ê³ , ì‹¤ìŠµì„ í†µí•´ ì‹¤ì œ ì‚¬ë¡€ì— ì ìš©í•´ë´…ë‹ˆë‹¤.

---

## 0. í•™ìŠµ ì‹œì‘: íŒŒìš´ë°ì´ì…˜ ëª¨ë¸

### íŒŒìš´ë°ì´ì…˜ ëª¨ë¸ì´ë€?
- ëŒ€ëŸ‰ì˜ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ì „ í•™ìŠµëœ ëŒ€ê·œëª¨ AI ëª¨ë¸
- ë‹¤ì–‘í•œ ì‘ì—…ì— ë²”ìš©ì ìœ¼ë¡œ í™œìš©í•  ìˆ˜ ìˆëŠ” ê¸°ì´ˆ(foundation) ì—­í• ì„ í•¨

### We will learn: íŒŒìš´ë°ì´ì…˜ ëª¨ë¸ì˜ ì£¼ìš” íŠ¹ì§•
- ê¸°ì¡´ AI ëª¨ë¸ê³¼ì˜ ì°¨ì´ì 
- 3ê°€ì§€ í•µì‹¬ êµ¬ì„±ìš”ì†Œ

### We will learn: í…ìŠ¤íŠ¸ íŒŒìš´ë°ì´ì…˜ ëª¨ë¸ (ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸)
- ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸ì˜ íŠ¹ì§•
- ëŒ€í‘œì ì¸ ì˜ˆì‹œ

---

## ì˜ˆì‹œ: í…ìŠ¤íŠ¸ ìƒì„± (ChatGPT)
- ChatGPTë¥¼ í™œìš©í•œ í…ìŠ¤íŠ¸ ìƒì„± ì˜ˆì‹œ

ì¶œì²˜: [https://openai.com/chatgpt/](https://openai.com/chatgpt/)
- <ê·¸ë¦¼0-1_í…ìŠ¤íŠ¸ íŒŒìš´ë°ì´ì…˜ ëª¨ë¸_íŒŒìš´ë°ì´ì…˜ ëª¨ë¸_ChatGPTì‚¬ìš© ì˜ˆì‹œ>

---

## ì˜ˆì‹œ: ë¹„ë””ì˜¤ ìƒì„± (SORA)
- í…ìŠ¤íŠ¸ ê¸°ë°˜ ë¹„ë””ì˜¤ ìƒì„± ì˜ˆì‹œ

ì¶œì²˜: [https://openai.com/index/sora/](https://openai.com/index/sora/)
- <ê·¸ë¦¼0-2_í…ìŠ¤íŠ¸ íŒŒìš´ë°ì´ì…˜ ëª¨ë¸_íŒŒìš´ë°ì´ì…˜ ëª¨ë¸_SORAë¥¼ í†µí•œ í…ìŠ¤íŠ¸ ê¸°ë°˜ ë¹„ë””ì˜¤ ìƒì„± ì˜ˆì‹œ>

---

## ì˜ˆì‹œ: ë©€í‹°ëª¨ë‹¬ ì…ë ¥ê³¼ ì¶œë ¥ (GPT-4o)
- ë©€í‹°ëª¨ë‹¬: ì´ë¯¸ì§€, ë¹„ë””ì˜¤, ì˜¤ë””ì˜¤, í…ìŠ¤íŠ¸

ì¶œì²˜: [https://openai.com/index/hello-gpt-4o/](https://openai.com/index/hello-gpt-4o/)
- <ê·¸ë¦¼0-3_í…ìŠ¤íŠ¸ íŒŒìš´ë°ì´ì…˜ ëª¨ë¸_íŒŒìš´ë°ì´ì…˜ ëª¨ë¸_GPT-4oë¥¼ í†µí•œ ì‹¤ì‹œê°„ ì§ˆì˜ ì‘ë‹µ ì˜ˆì‹œ>

---

## íŒŒìš´ë°ì´ì…˜ ëª¨ë¸ ì´ì „
- ìƒˆë¡œìš´ í…ŒìŠ¤í¬ë¥¼ í•´ê²°í•˜ë ¤ë©´ í•´ë‹¹ í…ŒìŠ¤í¬ì— ëŒ€í•œ â€œë³„ë„ì˜ í•™ìŠµâ€ í•„ìš”

ì˜ˆì‹œ:
- Early days [features] â†’ 0/1  
- AlexNet (2012) â†’ ship  
- BERT (2018) â†’ "i love this movie" â†’ pos/neg

ì¶œì²˜: [https://cs231n.stanford.edu/](https://cs231n.stanford.edu/)
- <ê·¸ë¦¼0-4_í…ìŠ¤íŠ¸ íŒŒìš´ë°ì´ì…˜ ëª¨ë¸_íŒŒìš´ë°ì´ì…˜ ëª¨ë¸_AI ëª¨ë¸ì„ í•™ìŠµí•˜ê¸° ìœ„í•œ ì´ì „ ë°©ë²•ë¡ ë“¤ì— ëŒ€í•œ ì˜ˆì‹œ>

---

## íŒŒìš´ë°ì´ì…˜ ëª¨ë¸ì˜ ë“±ì¥
- ìƒˆë¡œìš´ í…ŒìŠ¤í¬ë¥¼ í•´ê²°í•˜ë ¤ë©´ â€œìì„¸í•œ ì„¤ëª…(í”„ë¡¬í”„íŠ¸)â€ì„ ì…ë ¥í•´ì£¼ëŠ” ê²ƒìœ¼ë¡œ ì¶©ë¶„
- ChatGPT: í…ìŠ¤íŠ¸ íŒŒìš´ë°ì´ì…˜ ëª¨ë¸  
  (a.k.a ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸ or Large Language Model or LLM)

ì¶œì²˜: [https://www.gotai.co.kr/](https://www.gotai.co.kr/)
- <ê·¸ë¦¼0-5_í…ìŠ¤íŠ¸ íŒŒìš´ë°ì´ì…˜ ëª¨ë¸_íŒŒìš´ë°ì´ì…˜ ëª¨ë¸_í…ìŠ¤íŠ¸ íŒŒìš´ë°ì´ì…˜ ëª¨ë¸(ChatGPT)ì„ í†µí•œ ìƒˆë¡œìš´ í…ŒìŠ¤í¬(4í–‰ì‹œ) í•´ê²° ì˜ˆì‹œ>

## íŒŒìš´ë°ì´ì…˜ ëª¨ë¸
- ìƒˆë¡œìš´ í…ŒìŠ¤í¬ë¥¼ í•´ê²°í•˜ë ¤ë©´ â€œìì„¸í•œ ì„¤ëª…(í”„ë¡¬í”„íŠ¸)â€ì„ ì…ë ¥í•´ì£¼ëŠ” ê²ƒìœ¼ë¡œ ì¶©ë¶„ ğŸ˜Š  
  - SORA: ë¹„ë””ì˜¤ íŒŒìš´ë°ì´ì…˜ ëª¨ë¸

> â€œì„¸ë ¨ëœ ì—¬ì„±ì´ ë”°ëœ»í•˜ê²Œ ë¹›ë‚˜ëŠ” ë„¤ì˜¨ê³¼ ì›€ì§ì´ëŠ” ë„ì‹œ ê°„íŒìœ¼ë¡œ ê°€ë“í•œ ë„ì¿„ ê±°ë¦¬ë¥¼ ê±¸ì–´ ë‚´ë ¤ì˜¨ë‹¤.  
> ê·¸ë…€ëŠ” ê²€ì€ ê°€ì£½ ì¬í‚·, ê¸´ ë¹¨ê°„ ë“œë ˆìŠ¤, ê²€ì€ ë¶€ì¸ ë¥¼ ì°©ìš©í•˜ê³  ê²€ì€ ê°€ë°©ì„ ë“¤ê³  ìˆë‹¤.  
> ê·¸ë…€ëŠ” ì„ ê¸€ë¼ìŠ¤ë¥¼ ì“°ê³  ë¹¨ê°„ ë¦½ìŠ¤í‹±ì„ ë°”ë¥´ê³  ìˆë‹¤. ê·¸ë…€ëŠ”â€¦â€

ì¶œì²˜: https://openai.com/index/sora/

---

## íŒŒìš´ë°ì´ì…˜ ëª¨ë¸ì˜ 3ê°€ì§€ êµ¬ì„±ìš”ì†Œ

### (1) ë¹…ë°ì´í„°
- ì¸í„°ë„·ì— ì¡´ì¬í•˜ëŠ” ë°ì´í„° ìˆ˜ê°€ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ì¦ê°€

ì¶œì²˜: https://medium.com/@koshwemoethu.blacknet/the-era-of-big-data-863e87f0515d

---

### (1) ë¹…ë°ì´í„° (ì‹¬í™”)
- ë”¥ëŸ¬ë‹ ê¸°ë°˜ AI ëª¨ë¸ì€ í•™ìŠµ ë°ì´í„°ê°€ ëŠ˜ì–´ë‚ ìˆ˜ë¡ ì„±ëŠ¥ì´ ì¦ê°€

ì¶œì²˜: https://epochai.org/blog/trends-in-training-dataset-sizes

---

### (2) ìê°€ í•™ìŠµ(Self-supervised Learning)
- ì‚¬ëŒì´ ì •ë‹µì„ ì•Œë ¤ì¤„ í•„ìš” ì—†ìŒ
- ì˜ˆì‹œ: **ë‹¤ìŒ í† í° ì˜ˆì¸¡(Next token prediction)** ì„ í†µí•œ í…ìŠ¤íŠ¸ íŒŒìš´ë°ì´ì…˜ ëª¨ë¸(ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸) í•™ìŠµ

  - ì¸í„°ë„·ì—ì„œ ë°ì´í„° ì¶”ì¶œ â†’ â€œì•„ê¹Œ ë°¥ ë¨¹ê³  ì™”ì–´â€
  - í•™ìŠµ ë°ì´í„° ìƒì„±  
    - ì…ë ¥: ì•„ê¹Œ ë°¥ ë¨¹ê³   
    - ì •ë‹µ: ì™”ì–´  

ì¶œì²˜: https://tilnote.io/books/6480b090e92fe5ef635f54df/6480a73ee92fe5ef635f4d77

---

### (3) ì–´í…ì…˜(Attention) ê¸°ë°˜ íŠ¸ëœìŠ¤í¬ë¨¸(Transformer) ëª¨ë¸
- ë” ë§ì€ ë°ì´í„°ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ì¸ê³µì‹ ê²½ë§ êµ¬ì¡°  
  - **ì–´í…ì…˜(Attention)**: ì…ë ¥ ë°ì´í„°ì—ì„œ ì¤‘ìš”í•œ ë¶€ë¶„ì— ì£¼ì˜ë¥¼ ì§‘ì¤‘í•˜ëŠ” ë©”ì»¤ë‹ˆì¦˜  
  - **íŠ¸ëœìŠ¤í¬ë¨¸(Transformer)**: ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ì‹ ê²½ë§ êµ¬ì¡°  

ì¶œì²˜: https://www.researchgate.net/figure/Decoder-only-Transformer-architecture-The-input-to-the-decoder-is-tokenized-text-and_fig3_373183262

---

# 1. í…ìŠ¤íŠ¸ íŒŒìš´ë°ì´ì…˜ ëª¨ë¸ ì‚´í´ë³´ê¸°

## 1-1. í…ìŠ¤íŠ¸ íŒŒìš´ë°ì´ì…˜ ëª¨ë¸ì´ë€?

### íŒŒìš´ë°ì´ì…˜ ëª¨ë¸ì˜ 3ê°€ì§€ êµ¬ì„± ìš”ì†Œ in ì–¸ì–´ ëª¨ë¸ (Language Model)
- GPT-1, BERTì™€ ê°™ì€ ì–¸ì–´ ëª¨ë¸ì—ë„ 3ê°€ì§€ êµ¬ì„± ìš”ì†Œê°€ ì´ë¯¸ í¬í•¨ë˜ì–´ ìˆìŒ  
  - ê·¸ëŸ¬ë‚˜ íŒŒìš´ë°ì´ì…˜ ëª¨ë¸ê³¼ ê°™ì€ ëŠ¥ë ¥ì„ ë³´ì—¬ì£¼ì§€ ëª»í•¨ â†’ **ì–´ë–¤ ì°¨ì´ ë•Œë¬¸ì¼ê¹Œ?**

ì¶œì²˜: Delvin et al., *BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding*, NAACL 2019

---

### GPT-2: ì¶”ê°€ í•™ìŠµ ì—†ì´ ìƒˆë¡œìš´ í…ŒìŠ¤í¬ ìˆ˜í–‰ ê°€ëŠ¥
- ì–¸ì–´ ëª¨ë¸ì´ ì¶”ê°€ í•™ìŠµ ì—†ì´ë„ í…ìŠ¤íŠ¸ ì§€ì‹œë¥¼ í†µí•´ ìƒˆë¡œìš´ í…ŒìŠ¤í¬ë¥¼ â€œì–´ëŠ ì •ë„â€ ìˆ˜í–‰í•  ìˆ˜ ìˆìŒì„ í™•ì¸

ì¶œì²˜: Radford et al., *Language Models are Unsupervised Multitask Learners*, OpenAI 2019

## íŒŒìš´ë°ì´ì…˜ ëª¨ë¸ì˜ 3ê°€ì§€ êµ¬ì„± ìš”ì†Œ in ì–¸ì–´ ëª¨ë¸ (Language Model)
- GPT-2: ì–¸ì–´ ëª¨ë¸ì´ ì¶”ê°€ í•™ìŠµ ì—†ì´ë„ í…ìŠ¤íŠ¸ ì§€ì‹œë¥¼ í†µí•´ ìƒˆë¡œìš´ í…ŒìŠ¤í¬ë¥¼ â€œì–´ëŠ ì •ë„â€ ìˆ˜í–‰í•  ìˆ˜ ìˆìŒì„ í™•ì¸  
  - ê°€ì¥ í° GPT-2 ëª¨ë¸ì¡°ì°¨ë„ *underfitting*ëœ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤Œ â†’ **ëª¨ë¸ í¬ê¸°ë¥¼ ë” ëŠ˜ë ¸ì„ ë•Œ ì„±ëŠ¥ì´ ê°œì„ ë  ì—¬ì§€ê°€ ìˆìŒ**

$$
Perplexity = \sqrt[N]{\frac{1}{P(w_1, w_2, \ldots, w_N)}} = \sqrt[N]{\prod_{i=1}^{N} \frac{1}{P(w_i|w_1, w_2, \ldots, w_{i-1})}}
$$

ì¶œì²˜: Radford et al., *Language Models are Unsupervised Multitask Learners*, OpenAI 2019

---

## í…ìŠ¤íŠ¸ íŒŒìš´ë°ì´ì…˜ ëª¨ë¸(ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸)ì˜ íŠ¹ì´ì 

### (1) ê·œëª¨ì˜ ë²•ì¹™ (Scaling Law)
- ë” ë§ì€ ë°ì´í„°, í° ëª¨ë¸, ê¸´ í•™ìŠµ â†’ ë” ì¢‹ì€ ì„±ëŠ¥

ì¶œì²˜: Kaplan et al., *Scaling Laws for Neural Language Models*, arXiv:20.01

---

### (2) ì°½ë°œì„± (Emergent Property)
- íŠ¹ì • ê·œëª¨ë¥¼ ë„˜ì–´ì„œë©´ ê°‘ìê¸° ëª¨ë¸ì—ì„œ ë°œí˜„ë˜ëŠ” ì„±ì§ˆ  
  - **ì˜ˆì‹œ #1. ì¸-ì»¨í…ìŠ¤íŠ¸ í•™ìŠµ (In-context Learning)**  
    ì£¼ì–´ì§„ ì„¤ëª…ê³¼ ì˜ˆì‹œë§Œìœ¼ë¡œ ìƒˆë¡œìš´ í…ŒìŠ¤í¬ë¥¼ ìˆ˜í–‰  
  - **ì˜ˆì‹œ #2. ì¶”ë¡  (Reasoning)** ëŠ¥ë ¥  

ì¶œì²˜: Brown et al., *Language Models are Few-Shot Learners*, NeurIPS 2020

---

# 1-2. ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸ ì˜ˆì‹œ

## í…ìŠ¤íŠ¸ íŒŒìš´ë°ì´ì…˜ ëª¨ë¸ (or ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸, LLM)
- ê¸°ì¡´ ëŒ€ë¹„  
  1. ë” í° ëª¨ë¸(>7B)  
  2. ë” ë§ì€ ë°ì´í„°(>1T)ì—ì„œ í•™ìŠµë˜ì–´ ì°½ë°œì„±ì´ ë‚˜íƒ€ë‚˜ê¸° ì‹œì‘í•œ ì–¸ì–´ ëª¨ë¸  
- ì¼ë°˜ì ìœ¼ë¡œ ê±°ëŒ€ ì–¸ì–´ëª¨ë¸ì€ GPTì™€ ê°™ì´ ë‹¤ìŒ í† í° ì˜ˆì¸¡ì„ í†µí•´ ë§ì€ í…ìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ ì‚¬ì „ í•™ìŠµëœ íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ëª¨ë¸ì„ ì˜ë¯¸  

| êµ¬ë¶„ | ëŒ€í‘œ ëª¨ë¸ |
|------|------------|
| **íì‡„í˜• ê±°ëŒ€ì–¸ì–´ ëª¨ë¸** | ChatGPT, Claude 3, Gemini |
| **ê°œë°©í˜• ê±°ëŒ€ì–¸ì–´ ëª¨ë¸** | LLaMA 2, DeepSeek, Mistral AI |

ì¶œì²˜: SSAFY í…ìŠ¤íŠ¸ íŒŒìš´ë°ì´ì…˜ ëª¨ë¸ ìŠ¬ë¼ì´ë“œ

---

## íì‡„í˜• (Closed) ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸
- ì˜ˆì‹œ: ChatGPT (OpenAI), Claude (Anthropic), Gemini (Google)
- **ì¥ì :** ì¼ë°˜ì ìœ¼ë¡œ ë” ìš°ìˆ˜í•œ ì„±ëŠ¥ ë° ìµœì‹  ê¸°ëŠ¥ì„ ê°–ê³  ìˆìœ¼ë©° ì‚¬ìš©í•˜ê¸° ì‰¬ì›€  
- **ë‹¨ì :**  
  1. ì‚¬ìš© ì‹œë§ˆë‹¤ ë¹„ìš©ì´ ë°œìƒ  
  2. ëª¨ë¸ì´ë‚˜ ì¶œë ¥ì— ëŒ€í•œ ì •ë³´ê°€ ì œí•œì ìœ¼ë¡œ ì œê³µë¨  

ì¶œì²˜: https://platform.openai.com/docs/guides/text?lang=python  
https://openai.com/ko-KR/api/pricing/

# 2. ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸ì˜ í•™ìŠµ

## GPT-3: â€œê±°ëŒ€ ì–¸ì–´ ëª¨ë¸ì˜ ì‹œì´ˆâ€
- ê°€ì¥ í° ë²„ì „ì˜ GPT-3: **1750ì–µ ê°œì˜ ë§¤ê°œë³€ìˆ˜(Parameters)**  
  â†’ ì´ì „ ì–¸ì–´ ëª¨ë¸ ëŒ€ë¹„ ìµœì†Œ 10ë°° ì´ìƒ í° ëª¨ë¸  
- ë³¸ê²©ì ìœ¼ë¡œ **ì¸-ì»¨í…ìŠ¤íŠ¸ í•™ìŠµ(In-context learning)** ëŠ¥ë ¥ì´ ë‚˜íƒ€ë‚˜ê¸° ì‹œì‘í•œ ì–¸ì–´ ëª¨ë¸

ì¶œì²˜: https://medium.com/analytics-vidhya/openai-gpt-3-language-models-are-few-shot-learners-82531b3d3122

---

### í•™ìŠµ ë°©ë²• ë° ë¹„ìš©
- **í•™ìŠµ ë°©ë²•:** ë‹¤ìŒ í† í° ì˜ˆì¸¡ (Next token prediction)  
- **í•™ìŠµ ë°ì´í„°:** ì•½ 3000ì–µ í† í° (4TB í…ìŠ¤íŠ¸ ë°ì´í„° = ì¸í„°ë„· + ì–‘ì§ˆì˜ í…ìŠ¤íŠ¸ë¶)  
- **í•™ìŠµ ë¹„ìš©:** ì•½ **150ì–µ ì› ìˆ˜ì¤€**ìœ¼ë¡œ ì¶”ì‚°  

#### GPT-3 ëª¨ë¸ êµ¬ì„± ìš”ì•½
| Model | Parameters | Layers | Hidden Size | Attention Heads |
|--------|-------------|---------|--------------|----------------|
| GPT-3 Small | 125M | 12 | 768 | 12 |
| GPT-3 Medium | 350M | 24 | 1024 | 16 |
| GPT-3 Large | 760M | 24 | 1536 | 16 |
| GPT-3 XL | 1.3B | 24 | 2048 | 24 |
| GPT-3 2.7B | 2.7B | 32 | 2560 | 32 |
| GPT-3 6.7B | 6.7B | 32 | 4096 | 32 |
| GPT-3 13B | 13.0B | 40 | 5140 | 40 |
| **GPT-3 175B** | **175.0B** | **96** | **12288** | **96** |

#### í•™ìŠµ ë°ì´í„° êµ¬ì„±
| Dataset | Quantity (Tokens) | ë¹„ì¤‘(%) |
|----------|------------------|----------|
| Common Crawl (filtered) | 410B | 60 |
| WebText2 | 19B | 22 |
| Books1 | 12B | 8 |
| Books2 | 55B | 8 |
| Wikipedia | 3B | 3 |

ì¶œì²˜: Brown et al., *Language Models are Few-Shot Learners*, NeurIPS 2020

---

## ë‹¤ìŒ í† í° ì˜ˆì¸¡ ê¸°ë°˜ ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸ì˜ í•œê³„
- ì‚¬ëŒì˜ ì§€ì‹œì— ëŒ€í•´ **ì˜¬ë°”ë¥´ì§€ ì•Šì€ ì‘ë‹µ**ì„ ìƒì„±í•˜ê±°ë‚˜,  
  **ìœ í•´í•œ ì‘ë‹µ**ì„ ìƒì„±í•  ìˆ˜ ìˆìŒ.

ì˜ˆì‹œ:
- GPT-3: ì—‰ëš±í•œ ë‚´ìš© ìƒì„± (ë¬¸ë§¥ ë¬´ê´€í•œ ë¬¸ì¥ ë‚˜ì—´)
- GPT-4: ìœ í•´í•œ ì§€ì‹œì— ëŒ€í•´ ì‹¤í–‰ ë°©ë²•ì„ ì œì‹œí•˜ëŠ” ì˜¤ë¥˜ ë°œìƒ

ì¶œì²˜: Ouyang et al., *Training Language Models to Follow Instructions with Human Feedback*, NeurIPS 2022  
Wei et al., *Jailbroken: How Does LLM Safety Training Fail?*, NeurIPS 2023

---

## ì •ë ¬(Alignment) í•™ìŠµ
> ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸ì˜ ì¶œë ¥ì´ ì‚¬ìš©ìì˜ **ì˜ë„ì™€ ê°€ì¹˜**ë¥¼ ë°˜ì˜í•˜ë„ë¡ í•™ìŠµ

- (1) **ì§€ì‹œ í•™ìŠµ (Instruction tuning)**: ì£¼ì–´ì§„ ì§€ì‹œì— ëŒ€í•´ ì–´ë–¤ ì‘ë‹µì´ ìƒì„±ë˜ì–´ì•¼ í•˜ëŠ”ì§€ í•™ìŠµ  
- (2) **ì„ í˜¸ í•™ìŠµ (Preference learning)**: ìƒëŒ€ì ìœ¼ë¡œ ì–´ë–¤ ì‘ë‹µì´ ë” ì„ í˜¸ë˜ì–´ì•¼ í•˜ëŠ”ì§€ í•™ìŠµ  

ì¶œì²˜: Ouyang et al., *Training Language Models to Follow Instructions with Human Feedback*, NeurIPS 2022  
Wei et al., *Jailbroken*, NeurIPS 2023

---

# 2-1. ì§€ì‹œ í•™ìŠµ (Instruction Tuning)

## ì§€ì‹œ í•™ìŠµì˜ ê°œë…
> ì£¼ì–´ì§„ ì§€ì‹œì— ëŒ€í•´ ì–´ë–¤ ì‘ë‹µì´ ìƒì„±ë˜ì–´ì•¼ í•˜ëŠ”ì§€ë¥¼ í•™ìŠµ

- í•™ìŠµ ë°©ë²• ìì²´ëŠ” ê¸°ì¡´ ì–¸ì–´ ëª¨ë¸ (ì˜ˆ: BERT) ì—ì„œì˜ **ì§€ë„ í•™ìŠµ(Supervised Fine-Tuning, SFT)** ê³¼ ë™ì¼  
- ê¸°ì¡´ ì–¸ì–´ ëª¨ë¸ì€ ê° í…ŒìŠ¤í¬ë§ˆë‹¤ ë³„ë„ì˜ **ì¶”ê°€ í•™ìŠµ ë° ëª¨ë¸ ì €ì¥** í•„ìš”

ì˜ˆì‹œ:
> â€œì „ì²´ì ìœ¼ë¡œ, ë‘ ì‹œê°„ ë™ì•ˆ ì˜í™”ë¥¼ ë³´ëŠ” ê²ƒì—ì„œ ì–»ì€ ê°€ì¹˜ëŠ” íŒì½˜ê³¼ ìŒë£Œì˜ í•©ê³„ì˜€ìŠµë‹ˆë‹¤. ì˜í™”ëŠ” ì •ë§ ë”ì°í–ˆìŠµë‹ˆë‹¤.â€  
â†’ ì¶œë ¥: **0 (ë¶€ì •ì )**

---

## ì§€ì‹œ í•™ìŠµ (Instruction tuning): ì£¼ì–´ì§„ ì§€ì‹œì— ëŒ€í•´ ì–´ë–¤ ì‘ë‹µì´ ìƒì„±ë˜ì–´ì•¼ í•˜ëŠ”ì§€
- **Idea:** ëª¨ë“  ìì—°ì–´ í…ŒìŠ¤í¬ëŠ” í…ìŠ¤íŠ¸ ê¸°ë°˜ ì§€ì‹œ(instruction)ì™€ ì‘ë‹µìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆì§€ ì•Šì„ê¹Œ?

ì˜ˆì‹œ:
- ê°ì • ë¶„ì„: â€œì£¼ì–´ì§„ ë¦¬ë·° ì† ìœ ì €ì˜ ê°ì •ì´ ê¸ì •ì ì´ì•¼, ë¶€ì •ì ì´ì•¼?â€
- ë²ˆì—­: â€œì£¼ì–´ì§„ ë¬¸ì¥ì„ ì˜ì–´ë¡œ ë²ˆì—­í•´ì¤˜.â€

ì…ë ¥:  
> ì§€ì‹œ: â€œì£¼ì–´ì§„ ë¦¬ë·° ì† ìœ ì €ì˜ ê°ì •ì´ ê¸ì •ì ì´ì•¼, ë¶€ì •ì ì´ì•¼?â€  
> ë¦¬ë·°: â€œì „ì²´ì ìœ¼ë¡œ, ë‘ ì‹œê°„ ë™ì•ˆ ì˜í™”ë¥¼ ë³´ëŠ” ê²ƒì—ì„œ ì–»ì€ ê°€ì¹˜ëŠ” íŒì½˜ê³¼ ìŒë£Œì˜ í•©ê³„ì˜€ìŠµë‹ˆë‹¤. ì˜í™”ëŠ” ì •ë§ ë”ì°í–ˆìŠµë‹ˆë‹¤.â€  

ì¶œë ¥:  
> â€œë¶€ì •ì ìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤.â€

ì¶œì²˜: SSAFY í…ìŠ¤íŠ¸ íŒŒìš´ë°ì´ì…˜ ëª¨ë¸ ê°•ì˜ ìŠ¬ë¼ì´ë“œ

# 2-1. ì§€ì‹œ í•™ìŠµ

## ì§€ì‹œ í•™ìŠµ: ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸ì„ ë‹¤ì–‘í•œ ì§€ì‹œ ê¸°ë°˜ ì…ë ¥ê³¼ ì´ì— ëŒ€í•œ ì‘ë‹µìœ¼ë¡œ ì¶”ê°€í•™ìŠµ (FLAN)
- í•™ìŠµ ë°©ë²•: ì£¼ì–´ì§„ ì…ë ¥ì„ ë°›ì•„ì„œ ì´ì— ëŒ€í•œ ì‘ë‹µì„ ë”°ë¼ í•˜ë„ë¡ ì§€ë„ ì¶”ê°€ í•™ìŠµ (Supervised Fine-Tuning, SFT)
- (A) Pretrainâ€“finetune (BERT, T5)
  - Typically requires many task-specific examples
  - One specialized model for each task
- (B) Prompting (GPT-3)
  - Improve performance via few-shot prompting or prompt engineering
- (C) Instruction tuning (FLAN)
  - Model learns to perform many tasks via natural language instructions
  - Inference on unseen task
- ì¶œì²˜: Wei et al., *Finetuned Language Models Are Zero-Shot Learners*, ICLR 2022

## ì§€ì‹œ í•™ìŠµ: ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸ì„ ë‹¤ì–‘í•œ ì§€ì‹œ ê¸°ë°˜ ì…ë ¥ê³¼ ì´ì— ëŒ€í•œ ì‘ë‹µìœ¼ë¡œ ì¶”ê°€í•™ìŠµ (FLAN)
- í•™ìŠµ ë°ì´í„°ì˜ ë‹¤ì–‘ì„± ì¦ëŒ€ë¥¼ ìœ„í•´, ê° í…ŒìŠ¤í¬ë¥¼ ë‹¤ì–‘í•œ ì§€ì‹œ(í…œí”Œë¦¿)ë¡œ í‘œí˜„í•  ìˆ˜ ìˆìŒ
- Premise  
  `Russian cosmonaut Valery Polyakov set the record for the longest continuous amount of time spent in space, a staggering 438 days, between 1994 and 1995.`
- Hypothesis  
  `Russians hold the record for the longest stay in space.`
- Target  
  `Options: - yes  - no`
- Template ì˜ˆì‹œ
  - Template 1: Based on the paragraph above, can we conclude that `<hypothesis>`?
  - Template 2: Can we infer the following? `<hypothesis>`
  - Template 3: Read the following and determine if the hypothesis can be inferred from the premise  
    Premise: `<premise>`  
    Hypothesis: `<hypothesis>`  
    `<options>`
- ì¶œì²˜: Wei et al., *Finetuned Language Models Are Zero-Shot Learners*, ICLR 2022

## ì§€ì‹œ í•™ìŠµ: ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸ì„ ë‹¤ì–‘í•œ ì§€ì‹œ ê¸°ë°˜ ì…ë ¥ê³¼ ì´ì— ëŒ€í•œ ì‘ë‹µìœ¼ë¡œ ì¶”ê°€í•™ìŠµ (FLAN)
- ê¸°ì¡´ NLP í…ŒìŠ¤í¬ ë°ì´í„°ë¥¼ ì§€ì‹œ í•™ìŠµì„ ìœ„í•œ ë°ì´í„°ë¡œ ìˆ˜ì •í•˜ì—¬ í•™ìŠµ ë° í…ŒìŠ¤íŠ¸ì— í™œìš©
- í•™ìŠµì‹œì— ë³´ì§€ ëª»í•œ ì§€ì‹œì— ëŒ€í•œ ì¼ë°˜í™” ì„±ëŠ¥ í‰ê°€ë¥¼ ìœ„í•´, ê´€ë ¨ ì—†ëŠ” í…ŒìŠ¤í¬ë“¤ì„ í…ŒìŠ¤íŠ¸ì— ë³„ë„ë¡œ í™œìš©
  - ì˜ˆì‹œ: ìš”ì•½(`summarization`)ì„ í…ŒìŠ¤íŠ¸ ë•Œì˜ í…ŒìŠ¤í¬ë¡œ í™œìš©í•˜ê¸° ìœ„í•´, í•™ìŠµ ì‹œì—ëŠ” ì œê±°
- ì£¼ìš” ë°ì´í„°ì…‹
  - Natural language inference (7): ANLI(R1-R3), CB, MNLI, QNLI, RTE, SNLI, WNLI  
  - Commonsense (4): CoPA, HellaSwag, PiQA, StoryCloze  
  - Sentiment (4): IMDB, Sent140, SST-2, Yelp  
  - Paraphrase (4): MRPC, QQP, PAWS, STS-B  
  - Closed-book QA (3): ARC(easy/chal.), NQ, TQA  
  - Struct to text (4): CommonGen, DART, E2ENLG, WEBNLG  
  - Translation (8): ParaCrawl EN/DE, EN/ES, EN/FR, WMT-16 EN/CS, EN/DE, EN/FI, EN/RO, EN/RU, EN/TR  
  - Reading comp. (5): BoolQ, DROP, MultiRC, OBQA, SQuAD  
  - Read. comp. w/ commonsense (2): CosmosQA, ReCoRD  
  - Coreference (3): DPR, Winogrande, WSC273  
  - Misc. (7): CoQA, QuAC, WIC, Math, Fix Punctuation(NLG), TREC, CoLA  
  - Summarization (11): AESLC, AG News, CNN-DM, Gigaword, Multi-News, Newsroom, SamSum, Wiki Lingua EN, XSum, Opin-Abs: iDebate, Opin-Abs: Movie
- ì¶œì²˜: Wei et al., *Finetuned Language Models Are Zero-Shot Learners*, ICLR 2022

## ì§€ì‹œ í•™ìŠµ: ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸ì„ ë‹¤ì–‘í•œ ì§€ì‹œ ê¸°ë°˜ ì…ë ¥ê³¼ ì´ì— ëŒ€í•œ ì‘ë‹µìœ¼ë¡œ ì¶”ê°€í•™ìŠµ (FLAN)
- ì‹¤í—˜ ê²°ê³¼: ì˜ˆì‹œ ì—†ì´ë„ (`0-shot`) ìƒˆë¡œìš´ ì§€ì‹œì— ëŒ€í•´ ì˜¬ë°”ë¥¸ ì‘ë‹µì„ ë‚´ë†“ëŠ” ì„±ëŠ¥ì´ í¬ê²Œ ì¦ê°€!
- `LaMDA-PT 137B`ëŠ” êµ¬ê¸€ì˜ ë‹¹ì‹œ ê¸°ì¤€ `SOTA` í…ìŠ¤íŠ¸ íŒŒìš´ë°ì´ì…˜ ëª¨ë¸ì„ ì¶”ê°€ í•™ìŠµ
- ì£¼ìš” ë¹„êµ ëŒ€ìƒ: FLAN 137B, LaMDA-PT137B, GPT-3 175B, GLAM 64B/64E, Supervised model
- Natural language inference: ANLI R1-R3, CB, RTE  
- Reading comprehension: MultiRC, OBQA, BoolQ  
- Closed-book QA: NQ, ARC-C, TQA, ARC-e  
- Translation: ENâ†”RO, ENâ†”DE, ENâ†”FR  
- ì¶œì²˜: Wei et al., *Finetuned Language Models Are Zero-Shot Learners*, ICLR 2022

## ì§€ì‹œ í•™ìŠµ: íŒŒìš´ë°ì´ì…˜ ëª¨ë¸ì„ ë‹¤ì–‘í•œ ì§€ì‹œ ê¸°ë°˜ ì…ë ¥ê³¼ ì´ì— ëŒ€í•œ ì‘ë‹µìœ¼ë¡œ ì¶”ê°€í•™ìŠµ (FLAN or T0)
- ì‹¤í—˜ ê²°ê³¼: ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•œ í•µì‹¬ ìš”ì†ŒëŠ” ë‹¤ìŒê³¼ ê°™ìŒ
  1. í•™ìŠµ í…ŒìŠ¤í¬ì˜ ê°œìˆ˜: ë‹¤ì–‘í•œ ì¢…ë¥˜ì˜ ì§€ì‹œë¥¼ í•™ìŠµí• ìˆ˜ë¡ ë³´ì§€ ëª»í•œ ì§€ì‹œì— ëŒ€í•œ ì¼ë°˜í™” ì„±ëŠ¥ì´ ì¢‹ì•„ì§
  2. ì¶”ê°€ í•™ìŠµí•˜ëŠ” ëª¨ë¸ì˜ í¬ê¸°: íŠ¹ì • ê·œëª¨ ì´í•˜ì—ì„œëŠ” ì§€ì‹œ í•™ìŠµì˜ íš¨ê³¼ì„±ì´ ë–¨ì–´ì§ â†’ ì§€ì‹œë¥¼ ì´í•´í•˜ê³  ì‘ë‹µí•˜ëŠ” ê²ƒë„ ì°½ë°œì„±ì˜ í•˜ë‚˜
  3. ì§€ì‹œë¥¼ ì£¼ëŠ” ë°©ë²•: ìì—°ì–´ ì§€ì‹œë¡œ ì‚¬ëŒì—ê²Œ ëŒ€í™”í•˜ë“¯ ì§€ì‹œí•˜ëŠ” ê²ƒì´ ê°€ì¥ íš¨ê³¼ì 
- ì„±ëŠ¥ ë¹„êµ  
  - Held-out clusters: CommonSense / Average NLI / Closed-book QA  
  - FT: instruction / Eval: instruction (FLAN) â†’ `55.2`  
  - FT: dataset name / Eval: instruction â†’ `46.6`  
  - FT: dataset name / Eval: dataset name â†’ `47.0`  
  - FT: no instruction â†’ `37.3`
- ì¶œì²˜: Wei et al., *Finetuned Language Models Are Zero-Shot Learners*, ICLR 2022

# 2-2. ì„ í˜¸ í•™ìŠµ (Preference Learning)

## ì§€ì‹œ í•™ìŠµì˜ í•œê³„: ì£¼ì–´ì§„ ì…ë ¥ì— ëŒ€í•´ ì ì ˆí•œ í•˜ë‚˜ì˜ ì‘ë‹µì´ ìˆë‹¤ê³  ê°€ì •
- ì´ëŠ” ì •ë‹µì´ ì •í•´ì ¸ ìˆëŠ” ê°ê´€ì  í…ŒìŠ¤íŠ¸ (e.g. ìˆ˜í•™) ì—ì„œëŠ” ìì—°ìŠ¤ëŸ¬ì›€  
  - `Question`: ì–‘ì˜ ì •ìˆ˜ mê³¼ nì˜ ìµœëŒ€ê³µì•½ìˆ˜ëŠ” 6ì´ê³ , ìµœì†Œê³µë°°ìˆ˜ëŠ” 126ì´ë‹¤. ì´ë•Œ m + nì˜ ê°€ëŠ¥í•œ ìµœì†Œê°’ì€ ì–¼ë§ˆì¸ê°€?  
  - `Answer`: 60
- ì¶œì²˜: Cobbe et al., *Training Verifiers to Solve Math Word Problems*, OpenAI

## ì§€ì‹œ í•™ìŠµì˜ í•œê³„: ì£¼ì–´ì§„ ì…ë ¥ì— ëŒ€í•´ ì ì ˆí•œ í•˜ë‚˜ì˜ ì‘ë‹µì´ ìˆë‹¤ê³  ê°€ì •
- ì´ëŠ” ì •ë‹µì´ ì •í•´ì ¸ ìˆëŠ” ê°ê´€ì  í…ŒìŠ¤íŠ¸ (e.g. ìˆ˜í•™)ì—ì„œëŠ” ìì—°ìŠ¤ëŸ¬ì›€
- ê·¸ëŸ¬ë‚˜, ì •ë‹µì´ ì •í•´ì ¸ ìˆì§€ ì•Šì€ ê°œë°©í˜•(Open-ended) í…ŒìŠ¤íŠ¸ (e.g. ë²ˆì—­)ì—ì„œëŠ” í•œê³„ê°€ ìˆìŒ
  - Goal: ë‹¨ìˆœíˆ ë³µìˆ˜ ì •ë‹µì„ í—ˆë½í•˜ëŠ” ëŒ€ì‹ , ë” ì¢‹ì€(ì„ í˜¸ë˜ëŠ”) ì‘ë‹µì„ ìƒì„±í•˜ë„ë¡ í•˜ê³  ì‹¶ìŒ
- ì˜ˆì‹œ (ë²ˆì—­)
  - ì…ë ¥:  
    `When I am down and, oh my soul, so weary  
    When troubles come and my heart burdened be  
    Then, I am still and wait here in the silence  
    Until you come and still awhile with me`
  - ì¶œë ¥ 1:  
    `ë‚´ê°€ ìš°ìš¸í•˜ê³ , ì•„, ë‚´ ì˜í˜¼ì´ ë„ˆë¬´ ì§€ì³¤ì„ ë•Œ  
    ê³ ë‚œì´ ë‹¥ì¹˜ê³  ë‚´ ë§ˆìŒì´ ë¬´ê±°ì›Œì§ˆ ë•Œ  
    ê·¸ë•Œ, ë‚˜ëŠ” ì¡°ìš©íˆ ê¸°ë‹¤ë¦¬ë©° ì´ ì¹¨ë¬µ ì†ì— ë¨¸ë¬¼ëŸ¬ ìˆë„¤  
    ë‹¹ì‹ ì´ ì˜¤ì…”ì„œ ë‚˜ì™€ í•¨ê»˜ ì ì‹œ ë¨¸ë¬¼ëŸ¬ ì£¼ì‹¤ ë•Œê¹Œì§€`
  - ì¶œë ¥ 2:  
    `ë‚´ ë§ˆìŒì´ ì§€ì¹˜ê³  ì˜í˜¼ë§ˆì € ë¬´ê±°ìš¸ ë•Œ  
    ê·¼ì‹¬ì´ ì°¾ì•„ì™€ ê°€ìŠ´ì´ ì§“ëˆŒë¦´ ë•Œ  
    ë‚˜ëŠ” ì ì íˆ ì´ê³³ì—ì„œ ê¸°ë‹¤ë¦¬ë„¤  
    ê·¸ëŒ€ê°€ ì™€ì„œ ì ì‹œ ê³ì— ë¨¸ë¬¼ëŸ¬ ì£¼ê¸°ë¥¼`
- ì¶œì²˜: Cobbe et al., *Training Verifiers to Solve Math Word Problems*, OpenAI

## ì„ í˜¸ í•™ìŠµ (Preference Learning)
- ë‹¤ì–‘í•œ ì‘ë‹µ ì¤‘ ì‚¬ëŒì´ ë” ì„ í˜¸í•˜ëŠ” ì‘ë‹µì„ ìƒì„±í•˜ë„ë¡ ì¶”ê°€í•™ìŠµ
- ë‹¤ì–‘í•œ ì‘ë‹µì€ ëª¨ë¸ì´ ìƒì„±, ì‘ë‹µ ê°„ì˜ ì„ í˜¸ë„ëŠ” ì‚¬ëŒì´ ì œê³µ
- ChatGPTë¥¼ ë§Œë“¤ê¸° ìœ„í•œ í•µì‹¬ ì•Œê³ ë¦¬ì¦˜!
  - ê³µì‹ ë¬¸ì„œëŠ” ê³µê°œë˜ì–´ ìˆì§€ ì•Šì§€ë§Œ, ì•„ë˜ì™€ ê°™ì€ íŒíŠ¸ê°€ ê³µì‹ ë¸”ë¡œê·¸ì— ì œê³µë˜ì–´ ìˆìŒ
- ì¸ìš©:
```
We trained this model using Reinforcement Learning from Human Feedback (RLHF),
using the same methods as InstructGPT,
but with slight differences in the data collection setup.
We trained an initial model using supervised fine-tuning:
human AI trainers provided conversations in which they played both sidesâ€”the user and an AI assistant.
We gave the trainers access to model-written suggestions to help them compose their responses.
We mixed this new dialogue dataset with the InstructGPT dataset,
which we transformed into a dialogue format.
```
- ì¶œì²˜: https://openai.com/index/chatgpt/

# 2-2. ì„ í˜¸ í•™ìŠµ

## ì„ í˜¸ í•™ìŠµ (Preference Learning): ë‹¤ì–‘í•œ ì‘ë‹µ ì¤‘ ì‚¬ëŒì´ ë” ì„ í˜¸í•˜ëŠ” ì‘ë‹µì„ ìƒì„±í•˜ë„ë¡ ì¶”ê°€í•™ìŠµ
- InstructGPTì˜ í•µì‹¬ ì•„ì´ë””ì–´:
  - ì‚¬ëŒì˜ í”¼ë“œë°±ì„ í†µí•œ ê°•í™”í•™ìŠµ (Reinforcement Learning from Human Feedback, RLHF)
  - ì‚¬ëŒì˜ í”¼ë“œë°± := ì‘ë‹µì— ëŒ€í•œ ì„ í˜¸ë„
- Step 1: Collect demonstration data, and train a supervised policy.
- Step 2: Collect comparison data, and train a reward model.
- Step 3: Optimize a policy against the reward model using reinforcement learning.
- ì¶œì²˜: Ouyang et al., *Training Language Models to Follow Instructions with Human Feedback*, NeurIPS 2022

## InstructGPTì˜ í•™ìŠµ ë°©ë²•: Step 1. ì§€ì‹œ í•™ìŠµì„ í†µí•œ í…ìŠ¤íŠ¸ íŒŒìš´ë°ì´ì…˜ëª¨ë¸(e.g. GPT-3)ì˜ ì¶”ê°€ í•™ìŠµ
- ì‹¤ì œ ìœ ì €ë¡œë¶€í„° ë‹¤ì–‘í•œ ì§€ì‹œ ì…ë ¥ì„ ìˆ˜ì§‘í•˜ê³ , í•´ë‹¹ ì…ë ¥ì— ëŒ€í•´ í›ˆë ¨ëœ ì‚¬ëŒ ì£¼ì„ìë“¤ì´ ì •ë‹µ ë°ì´í„°ë¥¼ ìƒì„±
- Step 1: Collect demonstration data, and train a supervised policy.
  - A prompt is sampled from our prompt dataset.
  - A labeler demonstrates the desired output behavior.
  - This data is used to fine-tune GPT-3 with supervised learning.
- ì¶œì²˜: Ouyang et al., *Training Language Models to Follow Instructions with Human Feedback*, NeurIPS 2022

## InstructGPTì˜ í•™ìŠµ ë°©ë²•: Step 2. ì‚¬ëŒì˜ ì„ í˜¸ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì—¬, ë³´ìƒ ëª¨ë¸(Reward model, RM)ì„ í•™ìŠµ
- ì£¼ì–´ì§„ ì…ë ¥ì— ëŒ€í•œ ì„ íƒì§€ëŠ” ëª¨ë¸ì´ ìƒì„±, ë‹¤ì–‘í•œ ì„ íƒì§€ì— ëŒ€í•œ ì„ í˜¸ë„ëŠ” ì‚¬ëŒì´ ìƒì„±
- ì‚¬ëŒê³¼ ì¼ì¹˜í•œ ì„ í˜¸ë„ë¥¼ ì¶œë ¥í•  ìˆ˜ ìˆë„ë¡ ë³´ìƒ ëª¨ë¸ì„ ì§€ë„ í•™ìŠµ
  - ì‚¬ëŒì´ ì„ í˜¸í•˜ëŠ” ì‘ë‹µì´ ì…ë ¥ìœ¼ë¡œ ì£¼ì–´ì§ â†’ ë†’ì€ ë³´ìƒì„ ì¶œë ¥
- Step 2: Collect comparison data, and train a reward model.
  - A prompt and several model outputs are sampled.
  - A labeler ranks the outputs from best to worst.
  - This data is used to train our reward model.
- ì¶œì²˜: Ouyang et al., *Training Language Models to Follow Instructions with Human Feedback*, NeurIPS 2022

## InstructGPTì˜ í•™ìŠµ ë°©ë²•: Step 2. ì‚¬ëŒì˜ ì„ í˜¸ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì—¬, ë³´ìƒ ëª¨ë¸(Reward model, RM)ì„ í•™ìŠµ
- ì£¼ì–´ì§„ ì…ë ¥ì— ëŒ€í•œ ì„ íƒì§€ëŠ” ëª¨ë¸ì´ ìƒì„±, ë‹¤ì–‘í•œ ì„ íƒì§€ì— ëŒ€í•œ ì„ í˜¸ë„ëŠ” ì‚¬ëŒì´ ìƒì„±
- ì‚¬ëŒê³¼ ì¼ì¹˜í•œ ì„ í˜¸ë„ë¥¼ ì¶œë ¥í•  ìˆ˜ ìˆë„ë¡ ë³´ìƒ ëª¨ë¸ì„ ì§€ë„ í•™ìŠµ
  - ì‚¬ëŒì´ ì„ í˜¸í•˜ëŠ” ì‘ë‹µì´ ì…ë ¥ìœ¼ë¡œ ì£¼ì–´ì§ â†’ ë†’ì€ ë³´ìƒì„ ì¶œë ¥
- ì˜ˆì‹œ
  - ì„ íƒì§€ A: Explain gravity... â†’ Reward: `-5`
  - ì„ íƒì§€ D: People went to the moon... â†’ Reward: `100`
- ì¶œì²˜: Ouyang et al., *Training Language Models to Follow Instructions with Human Feedback*, NeurIPS 2022

## InstructGPTì˜ í•™ìŠµ ë°©ë²•: Step 3. ë³´ìƒì´ ë†’ì€ ì‘ë‹µì„ ìƒì„±í•˜ë„ë¡ ê°•í™” í•™ìŠµì„ í†µí•´ ì¶”ê°€ í•™ìŠµ
- í•µì‹¬: Step 1 & 2ì—ì„œ ë³´ì§€ ëª»í•œ ì§ˆë¬¸ì— ëŒ€í•´ ì‚¬ëŒì˜ ì¶”ê°€ì ì¸ ê°œì… ì—†ì´ í•™ìŠµëœ ëª¨ë¸ë“¤ì„ í†µí•´ ì¶”ê°€ í•™ìŠµì´ ì§„í–‰
- ì§€ì‹œ í•™ìŠµëœ ëª¨ë¸ì„ ë³´ìƒ ëª¨ë¸ ê¸°ë°˜ ê°•í™” í•™ìŠµì„ í†µí•´ í•œ ë²ˆ ë” ì¶”ê°€ í•™ìŠµ
- Step 3: Optimize a policy against the reward model using reinforcement learning.
  - A new prompt is sampled from the dataset.
  - The policy generates an output.
  - The reward model calculates a reward for the output.
  - The reward is used to update the policy using PPO.
- ì¶œì²˜: Ouyang et al., *Training Language Models to Follow Instructions with Human Feedback*, NeurIPS 2022

## InstructGPTì˜ ê²°ê³¼: ìœ ì €ì˜ ì§€ì‹œë¥¼ ì–¼ë§ˆë‚˜ ì˜ ìˆ˜í–‰í•˜ëŠ”ì§€ë¥¼ ì‚¬ëŒì´ ì§ì ‘ í‰ê°€
- ë‹¨ìˆœ í”„ë¡¬í”„íŒ…ì´ë‚˜ ì§€ì‹œ í•™ìŠµì— ë¹„í•´ ë°œì „ëœ ì§€ì‹œ ìˆ˜í–‰ëŠ¥ë ¥ì„ ë³´ì—¬ì¤Œ
  - InstructGPT: Likert score 5ì ëŒ€
  - Supervised Fine-Tuning: 4ì ëŒ€
  - GPT (prompted): 3ì ëŒ€
  - GPT: 2ì ëŒ€
- ì¶œì²˜: Ouyang et al., *Training Language Models to Follow Instructions with Human Feedback*, NeurIPS 2022

## InstructGPTì˜ ê²°ê³¼: ì–¼ë§ˆë‚˜ ì•ˆì „í•œ ì‘ë‹µì„ ìƒì„±í•˜ëŠ”ì§€ í‰ê°€
- ê¸°ì¡´ ëŒ€ë¹„, InstructGPTëŠ” í•´ë¡œìš´ ì‘ë‹µ(RealToxicity)ê³¼ ê±°ì§“ë§(TruthfulQA, Hallucinations)ì„ ëœ ìƒì„±
  - RealToxicity  
    GPT: `0.233`  
    SFT: `0.199`  
    InstructGPT: `0.196`
  - TruthfulQA  
    GPT: `0.224`  
    SFT: `0.206`  
    InstructGPT: `0.413`
  - Hallucinations  
    GPT: `0.414`  
    SFT: `0.078`  
    InstructGPT: `0.172`
- ì¶œì²˜: Ouyang et al., *Training Language Models to Follow Instructions with Human Feedback*, NeurIPS 2022

## LLaMA2
- InstructGPTì™€ ë¹„ìŠ·í•˜ê²Œ RLHFì™€ ëŒ€í™” ë°ì´í„°ë¥¼ í™œìš©í•œ LLaMA2 Chat ëª¨ë¸ì„ ê³µê°œ
  - LLaMA1 ë•Œì—ëŠ” ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ë§Œ ê³µê°œ
- êµ¬ì„±
  - Human feedback: Human preference data â†’ Helpful & Safety Reward Model ìƒì„±
  - Fine-tuning: RLHF, Rejection Sampling, Proximal Policy Optimization
  - Pretraining: Self-supervised learning ê¸°ë°˜ LLaMA2 í•™ìŠµ
  - ìµœì¢… ëª¨ë¸: LLaMA-2-chat
- ì¶œì²˜: Touvron et al., *Llama 2: Open Foundation and Fine-Tuned Chat Models*, Meta AI

## LLaMA2
- InstructGPTì™€ ë¹„ìŠ·í•˜ê²Œ RLHFì™€ ëŒ€í™” ë°ì´í„°ë¥¼ í™œìš©í•œ LLaMA2 Chat ëª¨ë¸ì„ ê³µê°œ
- ë‹¹ì‹œ ëŒ€í™”í˜• íƒ€ì… ê°œë°©í˜• ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸ ì¤‘ ê°€ì¥ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì¤Œ
- ì„±ëŠ¥ ë¹„êµ (% Win Rate)
  - LLaMA-2-70b-chat vs ChatGPT-0301 â†’ Win: `35.9`, Tie: `31.5`, Loss: `32.5`
  - LLaMA-2-70b-chat vs PaLM-Bison â†’ Win: `53.0`, Tie: `24.6`, Loss: `22.4`
  - LLaMA-2-34b-chat vs Falcon-40b-instruct â†’ Win: `76.3`, Tie: `14.6`, Loss: `9.1`
  - LLaMA-2-34b-chat vs Vicuna-33b-v1.3 â†’ Win: `37.2`, Tie: `31.2`, Loss: `31.2`
  - LLaMA-2-13b-chat vs Vicuna-13b-v1.1 â†’ Win: `45.4`, Tie: `29.8`, Loss: `24.9`
  - LLaMA-2-7b-chat vs MPT-7b-chat â†’ Win: `61.1`, Tie: `20.9`, Loss: `18.0`
- ì¶œì²˜: Touvron et al., *Llama 2: Open Foundation and Fine-Tuned Chat Models*, Meta AI

# 3. ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸ì˜ ì¶”ë¡ 

57p ë¶€í„°