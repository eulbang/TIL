# 251015
#### Q. 워드 임베딩과 원-핫 인코딩의 가장 큰 차이점은?
1) 원-핫 인코딩은 단어 수만큼의 차원을 갖는 희소벡터이고, 워드 임베딩은 저차원의 밀집 벡터 이다.
2) 원-핫 인코딩은 단어 간 의미 유사성을 잘 반영하지만, 워드 임베딩은 반영하지 못한다
3) 워드 임베딩은 항상 단어 개수와 동일한 차원을 가진다.
4) 원-핫 인코딩은 반드시 학습을 통해서만 생성되지만, 워드 임베딩은 학습이 필요 없다.
5) 워드 임베딩은 단어를 구분하기 위한 용도로만 쓰이고, 원-핫 인코딩은 단어 간 관계 분석에도 활용한다.

[정답 및 해설] 답은 1번.
- 원-핫 인코딩은 단어 집합의 크기 (어휘 수, Vocabulary size)만큼 차원을 만들고, 해당 단어 위치에만 1, 나머지는 0 으로 채운다.
- 워드 임베딩은 학습을 통해 단어를 100차원, 300차원 등 비교적 낮은 차원으로 압축하면서, 단어 간의 의미적 유사성을 반영한다.
- 따라서 차원의 크기와 표현 방식 (최소 vs 밀집)이 두 방법의 가장 큰 차이점이다.

#### Q. 다음 말뭉치 정보를 기반으로 4-gram 언어모델을 적용했을 때, 아래 문장에서 빈칸에 올 단어의 확률이 가장 높은 것은 무엇인가?
The SSAFY students learn ___

[말뭉치 정보]
- SSAFY students learn (900 번 등장)
- SSAFY students learn algorithm (270 번 등장)
- SSAFY students learn AI (450 번 등장)
- SSAFY students learn python (180 번 등장)

[보기] 
1) algorithm 
2) AI 
3) python 
4) database

[정답 및 해설] 답은 2번. 

4-gram 모델에서는 직전 3개 단어 (SSAFY students learn) 을 조건으로 다음 단어를 예측한다.

P(w|SSAFY students study) = count(SSAFY students study w) / count(SSAFY students study)

1) P(algorithm|SSAFY students study) = 270 / 900 = 0.3
2) P(AI|SSAFY students study) = 450 / 900 = 0.5
3) P(python|SSAFY students study) = 180 / 900 = 0.2
4) 말뭉치 정보에 database 는 주어지지 않음

#### Q. Seq2Seq 모델의 구조와 작동 방식에 대한 설명으로 가장 적절한 것은?
1) 인코더는 출력 시퀀스를 생성하고, 디코더는 입력 시퀀스를 처리한다.
2) 인코더와 디코더는 서로 독립적으로 학습되며, 따로 최적화된다.
3) 인코더는 입력 시퀀스를 고정 길이 벡터로 변환하고, 디코더는 이 벡터를 바탕으로 출력 시퀀스를 생성한다.
4) Seq2Seq 는 입력과 출력의 길이가 반드시 동일해야 하는 모델이다.

[정답 및 해설] 답은 3번.

Seq2Seq 모델은 인코더-디코더 구조로 이루어져 있다.
- 인코더: 입력 시퀀스를 한 타임스텝씩 읽어 고정된 차원의 벡터 표현 (CONTEXT)으로 변환한다.
- 디코더: 인코더에서 생성된 벡터를 조건으로 하여 출력 시퀀스를 생성한다.
- End-to-End 학습: 역전파를 통해 인코더와 디코더가 통합적으로 최적화한다.
- 가변 길이 처리: 입력과 출력의 길이가 달라도 처리 가능하다.

#### Q. Attention 메커니즘의 가장 핵심적인 아이디어는 무엇인가?
1) 인코더를 더 크게 만들어서 더 많은 정보를 저장한다.
2) 번역할 때마다 입력 문장의 어느 부분이 중요한지 선택적으로 집중한다.
3) 디코더를 여러 개 사용해서 병렬로 번역한다.
4) 입력 문장을 더 작은 단위로 나누어서 처리한다.

[정답 및 해설] 답은 2번.
Attention 은 문장을 번역할 때 "어느 부분이 중요한지" 스스로 판단한다. 매번 선택적으로 집중하는 것이 Attention 의 핵심 아이디어이다.

#### Q. Skip-gram 과 CBOW의 차이점은 무엇인가?
1) CBOW 는 항상 더 높은 정확도를 보장하고, Skip-gram은 속도만 빠르다.
2) Skip-gram 은 입력 차원이 항상 1이고, CBOW는 입력 차원이 단어 수와 같다.
3) CBOW 는 주변 단어 (context)로 중심 단어(Target)를 예측하고, Skip-gram 은 중심단어로 주변 단어를 예측한다.
4) CBOW 는 최소 벡터를 사용하고, Skip-gram 은 밀집 벡터만 사용한다.
5) Skip-gram 은 지도학습이도, CBOW 는 비지도학습이다.

[정답 및 해설] 답은 3번. 가장 큰 차이점은 예측 방향에 있다.
- CBOW (Continuous Bag of Words)
    - 장점: 학습 속도가 빠르고 데이터가 적을 때 효과적
    - 예) "The ___ is barking" 주변 단어 ["The", "is", "barking"]로 중심 단어 "dog" 예측
- Skip-gram
    - 장점: 드문 단어 (rare words) 학습에 더 효과적
    - 예) "dog" 주변 단어 ["The", "is", "barking"] 예측

#### Q. RNN 이 기존 신경망(FNN, CNN)과 다른 점은 무엇인가?
1) RNN 은 은닉층이 전혀 존재하지 않고, 입력층과 출력층만으로 구성된다.
2) RNN 은 입력 데이터를 항상 고정된 길이의 벡터로만 처리할 수 있다.
3) RNN 은 이미지를 인식하기 위해 컨볼루션 필터를 사용한다.
4) RNN 은 순환 구조를 가지고 있어 Sequence 데이터의 시간적 의존성을 학습할 수 있다.
5) RNN 은 순차 데이터 대신 오직 정적인 데이터 (예: 고정된 이미지) 만 처리할 수 있다.

[정답 및 해설] 답은 4번.
- FNN, CNN 은 입력에서 출력 방향으로만 흐르는 구조로, 입력 간 순서나 시간적 관계를 고려하지 못한다.
- RNN 은 은닉 상태를 다음 단계로 넘기는 순환 구조를 가지며, 이전 시점의 정보를 기억해 현재 출력에 반영할 수 있다.
- 따라서 자연어 처리 (NLP), 음성 인식, 시계열 데이처 예측처럼 순서 (temporal order)가 중요한 데이터에 적합하다.

#### Q. Self-Attention 의 설명에 해당 하는 것을 고르시오.
1) 시퀀스를 시간 순서대로 처리하며, 이전 시점의 은닉 상태를 다음 시점으로 전달해 의존성을 학습한다.
2) 동일한 시퀀스로부터 쿼리(Q), 키(K), 값(V)를 생성하고, 토큰 간 유사도에 기반한 가중합으로 각 토큰을 문맥화한다.
3) Positional Encoding, Residual connection, Layer Normalization, Feed-Forward Network 를 쌓아 시퀀스를 병렬 처리하는 아키텍쳐이다.
4) Word2Vec의 한 방식으로, 주변 단어 (컨텍스트)로 중심 단어를 예측하여 단어 임베딩을 학습한다.

[정답 및 해설] 답은 2번.
1) RNN 에 대한 설명
3) Transformer 에 대한 설명
4) CBOW 에 대한 설명

#### Q. Attention한계와 해결법을 짝지어 보라.
1. 순서 정보 부재       A. Feed-Forward Network 추가
2. 비선형성 부족        B. Masked Self-Attention
3. 미래 참조 문제       C. Positional Encoding

[정답 및 해설]
1-C. Self-Attention 은 입력 토큰의 순서를 직접 활용하지 않는다. 따라서 위치 정보가 없으면 토큰 순서를 바꿔도 같은 결과가 ??????
이를 보완하기 위해 Positional Encoding/Embedding 을 입력에 더하거나 결합해 위치 정보를 주입한다.

2-A. 가중합은 선형 결합이므로 복잡한 비선형 변환 능력이 제한된다. 이를 해결하기 위해 어텐션 뒤에 각 토큰 위치에 독립적으로 ??????
비선형 FFN 을 배치한다.

3-B. 왼쪽에서 오른쪽으로 예측할 때, 일반 Self-Attention 은 미래 토큰까지 볼 수 있어 정보 누구 (look ahead)가 발생한다. 
?????? -inf 를 부여해 softmax 가중치가 0이 되도록 마스킹하여 이를 차단한다.


#### Q. Transformer 모델의 Attention Head 의 개수가 늘어날 수록 성능이 무한히 향상되는가? (O/X)

[정답 및 해설] 답은 X.
멀티헤드는 다양한 관계를 병렬로 포착하지만, 헤드 수가 일정 수준을 넘으면 유사한 패턴만 반복 학습하는 "중복 헤드'가 늘어나기 쉽다. 또한 모델 성능은 학습 데이터 규모, 컨텍스트 길이, 최적화 설정 등 여러 요인의 영향을 받으므로, 성능이 가장 잘 나오는 최적의 헤드 수가 ???? 존재한다. 그 임계를 넘어가면 오히려 역효과가 날 수 있다.


# 251016
CNN 

이미지 입력이 3 x 32 x 32 로 들어요면, convolution filter 는 3 x 작은값 x 작은값 과 같이, 깊이/채널 축의 차원이 동일해야 한다.

- Activation map (a.k.a. feature map) 크기 계산하는 법
    - 예) 크기가 3 x 32 x 32 인데 0으로 padding 을 3씩 한 입력 이미지에, 크기가 3 x 5 x 5 이면서 stride 가 3인 합성곱 필터 6개를 쓸 때, 활성화 맵의 width, height 크기 (공간 해상도)는, {(입력 해상도)-(필터 해상도)+2*(padding 수)}/stride + 1 = (32-5+2*3)/3 + 1 = 12이다. 따라서 출력 shape 은 6 x 12 x 12 이다.
 
- 곱하기만 고려한 FLOPs (연산량) 구하는 법
    - 예) activation map 크기 구하는 예시를 예로 들면, FLOPs 는, (출력 세로) x (출력 가로) x (입력 채널 수) x (커널 사이즈) x (커널 사이즈) x (출력 채널 수) = 12 x 12 x 3 x 5 x 5 x 6 =  64800M FLOPs


